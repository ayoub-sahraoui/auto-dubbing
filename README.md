# ğŸ¬ Auto-Dubbing SaaS

A powerful web application for automatically dubbing videos in multiple languages using AI-powered transcription and text-to-speech technology.

![Auto-Dubbing Demo](https://img.shields.io/badge/Status-Production%20Ready-brightgreen)
![License](https://img.shields.io/badge/License-MIT-blue)

## âœ¨ Features

### ğŸ¯ Core Features
- **ğŸ¤ AI-Powered Transcription**: Utilizes OpenAI's Whisper large-v3-turbo model for accurate speech-to-text
- **ğŸŒ Multi-Language Support**: Transcribe and dub in English, French, Japanese, Chinese, and more
- **ğŸ—£ï¸ High-Quality TTS**: Kokoro-ONNX text-to-speech with multiple voice options
- **âœï¸ Interactive Transcript Editor**: Edit transcripts segment-by-segment or view/copy as SRT format
- **ğŸ¬ Automated Video Processing**: Seamless audio extraction, replacement, and video merging
- **ğŸ“Š Real-Time Progress**: Live progress tracking for all processing steps
- **ğŸ’¾ SRT Export**: Download standard SubRip subtitle files

### ğŸ¨ UI/UX Features
- Modern, responsive design with Tailwind CSS
- Dark/Light theme support
- Intuitive step-by-step workflow
- Real-time job status monitoring
- Beautiful animations and transitions

## ğŸ› ï¸ Tech Stack

### Frontend
- **Framework**: React 19 with TypeScript
- **Build Tool**: Vite 7
- **Styling**: Tailwind CSS 4
- **UI Components**: Radix UI, shadcn/ui
- **Icons**: Lucide React
- **State Management**: React Hooks

### Backend
- **Framework**: FastAPI (Python)
- **Server**: Uvicorn
- **AI Models**:
  - **Transcription**: Faster-Whisper (OpenAI Whisper)
  - **TTS**: Kokoro-ONNX
- **Video Processing**: FFmpeg (via ffmpeg-python)
- **Audio Processing**: SoundFile

## ğŸ“‹ Prerequisites

Before you begin, ensure you have the following installed:

### Required
- **Node.js** (v18 or higher) - [Download](https://nodejs.org/)
- **Python** (v3.10-3.12) - [Download](https://www.python.org/)
- **FFmpeg** - [Installation Guide](https://ffmpeg.org/download.html)
- **Git** - [Download](https://git-scm.com/)

### Recommended
- **CUDA-capable GPU** (NVIDIA) for faster transcription (optional but recommended)
- **16GB+ RAM** for processing large videos
- **10GB+ free disk space** for AI models and cache

### Verify Installation

```bash
# Check Node.js version
node --version  # Should be v18 or higher

# Check Python version
python --version  # Should be 3.10-3.12

# Check FFmpeg
ffmpeg -version  # Should show FFmpeg details

# Check npm
npm --version
```

## ğŸš€ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/auto-dubbing.git
cd auto-dubbing
```

### 2. Frontend Setup

```bash
# Install dependencies
npm install

# The installation might take a few minutes
```

### 3. Backend Setup

#### Windows

```powershell
# Navigate to backend directory
cd backend

# Create virtual environment
python -m venv venv

# Activate virtual environment
.\venv\Scripts\Activate.ps1

# If you get an execution policy error, run:
# Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# Install Python dependencies
pip install -r requirements.txt

# This will download AI models (~2-4GB) on first run
```

#### macOS/Linux

```bash
# Navigate to backend directory
cd backend

# Create virtual environment
python3 -m venv venv

# Activate virtual environment
source venv/bin/activate

# Install Python dependencies
pip install -r requirements.txt
```

### 4. FFmpeg Installation

#### Windows
1. Download from [FFmpeg Official Site](https://ffmpeg.org/download.html)
2. Extract to `C:\ffmpeg`
3. Add `C:\ffmpeg\bin` to your PATH environment variable
4. Restart your terminal/PowerShell

#### macOS (using Homebrew)
```bash
brew install ffmpeg
```

#### Linux (Ubuntu/Debian)
```bash
sudo apt update
sudo apt install ffmpeg
```

### 5. Configuration

#### Backend Configuration

Create or edit `backend/.env`:

```env
# Whisper Model Configuration
# Available models: tiny, base, small, medium, large-v3, large-v3-turbo
WHISPER_MODEL=large-v3-turbo

# Device: auto (auto-detect GPU), cpu, cuda
WHISPER_DEVICE=auto

# Compute type: auto, int8, float16, float32
WHISPER_COMPUTE_TYPE=auto

# Kokoro TTS Configuration
KOKORO_DEFAULT_VOICE=af_heart
KOKORO_DEFAULT_LANG=a

# CORS Configuration (Frontend URL)
CORS_ORIGINS=http://localhost:5173,http://localhost:3000
```

#### Environment Variables Explained

| Variable | Description | Default | Options |
|----------|-------------|---------|---------|
| `WHISPER_MODEL` | Whisper model size | `large-v3-turbo` | `tiny`, `base`, `small`, `medium`, `large-v3`, `large-v3-turbo` |
| `WHISPER_DEVICE` | Processing device | `auto` | `auto`, `cpu`, `cuda` |
| `WHISPER_COMPUTE_TYPE` | Computation precision | `auto` | `auto`, `int8`, `float16`, `float32` |
| `CORS_ORIGINS` | Allowed frontend URLs | `http://localhost:5173` | Comma-separated URLs |

## ğŸ® Running the Application

### Quick Start (Recommended)

You'll need **two terminal windows** - one for frontend, one for backend.

#### Terminal 1: Backend

```bash
# Navigate to backend directory
cd backend

# Activate virtual environment
# Windows:
.\venv\Scripts\Activate.ps1
# macOS/Linux:
source venv/bin/activate

# Start backend server
uvicorn main:app --reload

# Expected output:
# INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
# INFO:     Started reloader process
# INFO:     ğŸš€ Auto-Dubbing API starting up...
```

Backend will be available at: **http://localhost:8000**

#### Terminal 2: Frontend

```bash
# From project root
npm run dev

# Expected output:
# VITE v7.x.x  ready in xxx ms
# âœ  Local:   http://localhost:5173/
```

Frontend will be available at: **http://localhost:5173**

### First Run Notes

â³ **First run will be slower** because:
1. Whisper model will be downloaded (~1-2GB)
2. Kokoro TTS models will be downloaded (~300MB)
3. Models will be cached for future use

ğŸ“ Models are stored in:
- Whisper: `~/.cache/huggingface/` (Windows: `C:\Users\YourName\.cache\huggingface\`)
- Kokoro: `backend/` directory

## ğŸ“– Usage Guide

### Step-by-Step Workflow

#### 1. **Upload Video**
- Click "Choose File" or drag-and-drop your video
- Supported formats: MP4, MKV, AVI, MOV, WebM
- Maximum size: 500MB (configurable in `backend/config.py`)

#### 2. **Transcribe**
- Select source language (or use auto-detect)
- Click "Start Transcription"
- Wait for AI to transcribe the audio
- Progress bar shows current status

#### 3. **Review & Edit Transcript**
- **Segments Tab**: Edit individual transcript segments with timing
- **SRT Format Tab**: View/copy the transcript in SubRip format
- Click on any segment to edit text
- Press Enter or click outside to save changes

#### 4. **Generate Voiceover**
- Select target language
- Choose voice (gender and style)
- Adjust speed (0.5x - 2.0x)
- Click "Generate Voiceover"

#### 5. **Merge & Download**
- Click "Merge Video" to combine voiceover with original video
- Download your dubbed video
- Also download SRT subtitle file if needed

### Available Voices

**American English**
- af_heart, af_bella, af_nicole, af_sarah, af_sky (Female)
- am_adam, am_michael (Male)

**British English**
- bf_emma, bf_isabella (Female)
- bm_george, bm_lewis (Male)

**Japanese**
- jf_alpha, jf_gongitsune (Female)
- jm_kumo (Male)

**Mandarin Chinese**
- zf_xiaobei, zf_xiaoni, zf_xiaoxiao (Female)
- zm_yunjian (Male)

**French**
- ff_siwis (Female)

## ğŸ”§ Configuration & Optimization

### Performance Tuning

#### For Faster Transcription (Lower Accuracy)
Edit `backend/.env`:
```env
WHISPER_MODEL=base  # or small
```

#### For Better Accuracy (Slower)
Edit `backend/.env`:
```env
WHISPER_MODEL=large-v3
```

See `backend/WHISPER_ACCURACY_GUIDE.md` for detailed accuracy optimization.

### GPU Acceleration

If you have an NVIDIA GPU:

```bash
# Install CUDA toolkit first, then:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Update .env:
WHISPER_DEVICE=cuda
WHISPER_COMPUTE_TYPE=float16
```

**Performance improvement**: 5-10x faster transcription with GPU!

## ğŸ“ Project Structure

```
auto-dubbing/
â”œâ”€â”€ backend/                    # Python FastAPI backend
â”‚   â”œâ”€â”€ main.py                # FastAPI application entry
â”‚   â”œâ”€â”€ config.py              # Configuration settings
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â”œâ”€â”€ .env                   # Environment variables
â”‚   â”œâ”€â”€ routers/               # API route handlers
â”‚   â”‚   â””â”€â”€ dubbing.py         # Main dubbing workflow routes
â”‚   â”œâ”€â”€ services/              # Business logic services
â”‚   â”‚   â”œâ”€â”€ transcription.py   # Whisper transcription service
â”‚   â”‚   â”œâ”€â”€ tts.py            # Kokoro TTS service
â”‚   â”‚   â””â”€â”€ video.py          # FFmpeg video processing
â”‚   â”œâ”€â”€ models/                # Pydantic data models
â”‚   â”‚   â””â”€â”€ schemas.py         # Request/Response schemas
â”‚   â”œâ”€â”€ storage/               # Uploaded files and outputs
â”‚   â”‚   â”œâ”€â”€ uploads/           # Uploaded videos
â”‚   â”‚   â””â”€â”€ outputs/           # Generated dubbed videos
â”‚   â””â”€â”€ venv/                  # Python virtual environment
â”œâ”€â”€ src/                       # React frontend source
â”‚   â”œâ”€â”€ components/            # React components
â”‚   â”‚   â”œâ”€â”€ ui/               # shadcn/ui components
â”‚   â”‚   â”œâ”€â”€ TranscriptEditor.tsx  # Transcript editing interface
â”‚   â”‚   â”œâ”€â”€ VideoUpload.tsx   # Video upload component
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ api.ts                # API client functions
â”‚   â”œâ”€â”€ App.tsx               # Main application component
â”‚   â””â”€â”€ main.tsx              # Application entry point
â”œâ”€â”€ public/                    # Static assets
â”œâ”€â”€ index.html                # HTML entry point
â”œâ”€â”€ package.json              # Node.js dependencies
â”œâ”€â”€ vite.config.ts            # Vite configuration
â”œâ”€â”€ tailwind.config.js        # Tailwind CSS configuration
â””â”€â”€ README.md                 # This file
```

## ğŸ› Troubleshooting

### Common Issues

#### âŒ "FFmpeg not found"
**Solution**:
- Ensure FFmpeg is installed and added to PATH
- Restart your terminal after installation
- Test with `ffmpeg -version`

#### âŒ "ModuleNotFoundError: No module named 'xxx'"
**Solution**:
```bash
# Make sure virtual environment is activated
cd backend
.\venv\Scripts\Activate.ps1  # Windows
source venv/bin/activate      # macOS/Linux

# Reinstall dependencies
pip install -r requirements.txt
```

#### âŒ "CUDA out of memory" error
**Solution**:
- Use a smaller Whisper model: `WHISPER_MODEL=base`
- Or switch to CPU: `WHISPER_DEVICE=cpu`
- Reduce video resolution/length

#### âŒ "Port already in use"
**Solution**:
```bash
# Backend (Port 8000)
uvicorn main:app --reload --port 8001

# Frontend (Port 5173)
npm run dev -- --port 5174
```

#### âŒ Slow transcription on CPU
**Expected behavior**: CPU transcription is slower
- `base` model: ~1x realtime
- `large-v3-turbo`: ~0.3x realtime (3x slower than audio length)
- Consider using GPU or smaller model

#### âŒ "CORS policy" errors
**Solution**:
- Check `backend/.env` has correct frontend URL
- Ensure both frontend and backend are running
- Clear browser cache

### Check Installation

Run this diagnostic:

```bash
# Backend
cd backend
python -c "import faster_whisper; import kokoro_onnx; print('âœ… All packages installed')"

# Frontend
npm list react
```

## ğŸ“Š Performance Benchmarks

Typical processing times for a **5-minute video**:

| Hardware | Model | Transcription | Voice Generation | Total |
|----------|-------|---------------|------------------|-------|
| CPU (Intel i7) | large-v3-turbo | ~12-15 min | ~2 min | ~17 min |
| CPU (Intel i7) | base | ~3-5 min | ~2 min | ~7 min |
| GPU (RTX 3060) | large-v3-turbo | ~1-2 min | ~2 min | ~4 min |
| GPU (RTX 4090) | large-v3 | ~1 min | ~1 min | ~2 min |

## ğŸ”’ Security Notes

âš ï¸ **This is a development setup. For production:**

1. **Add authentication** to protect API endpoints
2. **Use HTTPS** for all connections
3. **Implement rate limiting** to prevent abuse
4. **Add file scanning** for uploaded videos
5. **Use environment secrets** management
6. **Set up proper CORS** policies
7. **Implement user quotas** for resource management

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [OpenAI Whisper](https://github.com/openai/whisper) for speech recognition
- [Faster-Whisper](https://github.com/SYSTRAN/faster-whisper) for optimized inference
- [Kokoro-ONNX](https://github.com/thewh1teagle/kokoro-onnx) for text-to-speech
- [FFmpeg](https://ffmpeg.org/) for video/audio processing
- [FastAPI](https://fastapi.tiangolo.com/) for the backend framework
- [React](https://react.dev/) for the frontend framework
- [shadcn/ui](https://ui.shadcn.com/) for beautiful UI components

## ğŸ“ Support

Having issues? Check out:
- [Troubleshooting](#-troubleshooting) section above
- [Whisper Accuracy Guide](backend/WHISPER_ACCURACY_GUIDE.md)
- [SRT Format Guide](SRT_FORMAT_VIEWER.md)
- [Backend Logging Info](backend/LOGGING_INFO.md)

## ğŸ—ºï¸ Roadmap

- [ ] Add more language support
- [ ] Implement user authentication
- [ ] Add batch processing
- [ ] Cloud deployment guides
- [ ] Docker containerization
- [ ] Real-time subtitle preview
- [ ] Custom voice training
- [ ] Multi-speaker detection
- [ ] Translation feature
- [ ] Progress notifications

---

Made with â¤ï¸ by the Auto-Dubbing Team

**Star â­ this repo if you find it useful!**
